---
title: "Traping bots on your server:"
date: 2025-08-05
tags: ["programming"]
---

Here's the code for my [infinite nonsense (demo)](https://maurycyz.com/babble/entry_point) crawler trap:

Markov chain creation script: [process.py](/projects/trap_bots/process.py) 

Server: [babble.c](/projects/trap_bots/babble.c)


## Training the Markov chain:

First, you'll want to find three long-ish sources of text, at least 1000 words. 
I used old books from [Project Guttenberg](https://www.gutenberg.org/), but longer blog posts or your favorate Wikipedia articles will work fine. 

Save the text in files named "book1", "book2" and "book3", and remove any page numbers, headings, branding or anything else that's not the main text. 

If your texts are somewhat short (less then 3000 words), change the "context\_length" varable in process.py to 1. 
This results in smaler output files and less repetive text. 
For longer bodies of text, leaving it at 2 usually produceds more coherent output.

Next, run the python script:

```sh
python3 process.py
```

It should produce files "chain1.txt" though "chain3.txt". 

## The server:

First, configure your web server to forward all requests under an unused path to `localhost:[some unused port]`.
If you use NGINX, add this to your "`nginx.conf`":

```py
# Proxy traffic to markov babbler
# Place inside a server { } block
location /babble/ {
	proxy_pass http://localhost:1414/;
}
```
<center style="color: gray">Other servers can do the same thing, but the syntax will differ</center>

Next, open the C code and change "PORT" to your unused port and "URL_PREFIX" to the path that the server will forward:

```c
...
#define PORT 1414
...
// Must begin and end with "/"s
const char* URL_PREFIX = "/babble/"
...
```

There are a few more options, but those two are all you should need to adjust before compiling:

```sh
gcc -O3 -lm -o babble babble.c
```

Now, run the binary in the same directory with the "chainX.txt" files:

```sh
./babble
```

It may take a while to read the markov chains if "context_length = 2" was used to generate them.
Once it finishes loading, you should be able to navigate to a URL under the URL_PREFIX and see garbage: 

```txt
https://your.domain.example.com/babble/test
```

If you see a 5XX error, make sure you have the right path and port in your web server's configuration.
If the page loads but the links are broken, check the "`URL_PREFIX`" and recompile

If everything works, you'll want to run the program in the background.
If you run systemd on Linux, create a file called:

```txt
/etc/systemd/system/babbler.service
```

... with the following contense: 
(don't forget to change the file paths)

```ini
[Unit]
Description=Markov chain based crawler trap
After=network.target
StartLimitIntervalSec=0

[Service]
Type=simple
Restart=always
RestartSec=1
User=nobody
WorkingDirectory=/path/to/markov/chains
ExecStart=/path/to/binary/babble
StandardOutput=null

[Install]
WantedBy=multi-user.target
```

... and run these commands as root:

```sh
systemctl daemon-reload
systemctl start babble
systemctl enable babble
```

## Feeding it:

You don't really need any bot detection, just a single link to the garbage from your website will do.
Because each page links to five more garbage pages, the crawler's queue will quickly fill up with an exponential amount of garbage until it has no time left to crawl your real site.

If you don't want your site to disapear from search results, exclude the garbage urls in "/robots.txt":

```txt
User-agent: Googlebot Bingbot Kagibot
Disallow: /babble/*
```

If you want prevent bots from ever seeing any of your content:
[ai.robots.txt](https://github.com/ai-robots-txt/ai.robots.txt/tree/main) has nice lists of User-Agents. 
Just change them to URL-rewrite or redirect instead of blocking the request.

Here's what that looks like NGINX:

```py
# Send scrapers garbage
# Regex from https://github.com/ai-robots-txt/ai.robots.txt/blob/main/nginx-block-ai-bots.conf
if ($http_user_agent ~* "(AddSearchBot|AI2Bot|Ai2Bot\-Dolma|aiHitBot|Amazonbot|Andibot|anthropic\-ai|Applebot|Applebot\-Extended|Awario|bedrockbot|bigsur\.ai|Brightbot\ 1\.0|Bytespider|CCBot|ChatGPT\ Agent|ChatGPT\-User|Claude\-SearchBot|Claude\-User|Claude\-Web|ClaudeBot|CloudVertexBot|cohere\-ai|cohere\-training\-data\-crawler|Cotoyogi|Crawlspace|Datenbank\ Crawler|Devin|Diffbot|DuckAssistBot|Echobot\ Bot|EchoboxBot|FacebookBot|facebookexternalhit|Factset_spyderbot|FirecrawlAgent|FriendlyCrawler|Gemini\-Deep\-Research|Google\-CloudVertexBot|Google\-Extended|GoogleAgent\-Mariner|GoogleOther|GoogleOther\-Image|GoogleOther\-Video|GPTBot|iaskspider/2\.0|ICC\-Crawler|ImagesiftBot|img2dataset|ISSCyberRiskCrawler|Kangaroo\ Bot|LinerBot|meta\-externalagent|Meta\-ExternalAgent|meta\-externalfetcher|Meta\-ExternalFetcher|MistralAI\-User|MistralAI\-User/1\.0|MyCentralAIScraperBot|netEstate\ Imprint\ Crawler|NovaAct|OAI\-SearchBot|omgili|omgilibot|Operator|PanguBot|Panscient|panscient\.com|Perplexity\-User|PerplexityBot|PetalBot|PhindBot|Poseidon\ Research\ Crawler|QualifiedBot|QuillBot|quillbot\.com|SBIntuitionsBot|Scrapy|SemrushBot\-OCOB|SemrushBot\-SWA|Sidetrade\ indexer\ bot|Thinkbot|TikTokSpider|Timpibot|VelenPublicWebCrawler|WARDBot|Webzio\-Extended|wpbot|YaK|YandexAdditional|YandexAdditionalBot|YouBot)") {
	rewrite ^(.*)$ /babble/$1;
}
# Also send garbage to firefox's ai summaries
if ($http_x-firefox-ai ~* "1") {
	rewrite ^(.*)$ /babble/$1;
}
```

This will also ensure that AI chatbots or summarizers only ever get garbage...
after all, the easiest thing to turn into garbage is garbage: *See, we're just helping!*

Just beware that there is a sigificant amount of scraping using residential IP and browser User-Agents, so adding a link is recomended even if you filter by User-Agent.
